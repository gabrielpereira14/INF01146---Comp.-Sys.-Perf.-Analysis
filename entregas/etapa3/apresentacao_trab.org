#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LATEX_CLASS: beamer
#+LATEX_HEADER: \setbeamertemplate{itemize item}{\textbullet}
#+LATEX_HEADER: \setbeamertemplate{itemize subitem}{\textbullet}
#+LATEX_HEADER: \setbeamertemplate{itemize subsubitem}{\textbullet}
#+LATEX_HEADER: \setbeamertemplate{headline}{}
#+LATEX_HEADER: \setbeamertemplate{footline}{}
#+LATEX_HEADER: \setbeamertemplate{navigation symbols}{}
#+BEAMER_FRAME_LEVEL: 2
#+PROPERTY: header-args :python :var ORGDIR=(file-name-directory buffer-file-name) :exports results  

#+TITLE: Medição do impacto de uma infraestrutura de VPN na taxa de transmissão e na latência
#+AUTHOR: Ester Crestani, Gabriel Pereira e Júlia Pimentel
#+DATE: 30/11/2025
#+OPTIONS: toc:nil H:2 eval

* Medição do impacto da infraestrutura de VPN da UFRGS na taxa de transmissão e na latência :noexport:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:

  O objetivo deste trabalho é avaliar o impacto da infraestrutura de VPN da UFRGS, utilizada através do OpenVPN, sobre o throughput, latência, jitter e perda de pacotes.

** Ferramentas utilizadas 
  - iperf para a coleta de throughput;
  - ping para a coleta de latência, jitter e perda de pacotes.

** Metodologia 
  Foram desenvolvidos dois scripts para a coleta de dados. O principal é o run_test.py, ele contém as funções que iniciam e param a execução da VPN, além de 
  chamar o script que faz os experimentos do ping. Também contém as funções para a execução dos testes do iperf. Tentou-se separar a parte do iperf em um terceiro script para
  manter uma melhor modularização, entretanto 
  - Execução simultânea de ping e iperf;
    
** Coleta dos dados 
  Para fazer a coleta dos dados, foram feitos 3 scripts:
    - "run_test.py" - é o script principal, é responsável pelo controle da VPN, bem como usa o script que coleta os dados do ping e coleta dados do ipef.
    - "run_ping_test.py" - coleta os dados do ping, foi feito em um outro arquivo para possibilitar a paralelização da coleta. 
    - "utils/env_loader" - le os dados do arquivo .env que define o nome da pasta onde salvar os dados coletados
    - "utils/csv_gzip_manager.py" - define as funcões que tratam de ler descompactar e compactar os dados no inicio e fim da coleta 
  
**** Como executar

***** Configuração do ambiente
  Garanta que as bibliotecas python necessárias estão instaladas, elas estão descritas no arquivo "requirements.txt" no diretório src, e podem ser facilmente instaladas com o comando:

`pip install -r requirements.txt`

  onde 'path' é o caminho para o arquivo "requirements.txt".

  Também é necessário criar um arquivo .env na raiz do projeto, ela deve conter a váriavel USER_NAME, por exemplo:
 
  `USER_NAME=meu_nome`

  com isso, os dados da coleta serão salvos em data/meu_nome.

  Por fim, também é necessario criar um arquivo chamado "pass.txt" no diretorio utils, este será o arquivo que conterá as suas credenciais de acesso à VPN da UFRGS (mesmas credenciais do moodle), neste formato:
   
  ```
  00112233
  minhasenhadomoodle
  ```

  Também é necessario executar o servidor do iperf no PCAD, usando: 

  ```
  iperf -s -p 8787
  ```

***** Executando a coleta
  Como o OpenVPN precisa ser rodado com sudo, para o nosso grupo foi mais fácil rodar o script de coleta diretamente pelo terminal do que usando o emacs. Dessa forma,
  deixaremos o conteudo dos scripts neste arquivo e o comando necessário para executá-los no terminal do sistema operacional.

  #+BEGIN_SRC python :exports :results output
    import os
    SCRIPT_DIR = ORGDIR 
    PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, "..", ".."))

    print(f"python3 {PROJECT_ROOT}/src/run_tests.py")
  #+END_SRC

  #+RESULTS:
  : python3 /home/gabip/perf/src/run_tests.py

  Com esse comando é possível executar o script de coleta a partir do terminal

  A seguir estarão os scripts usados na coleta.

***** Script principal - run_tests.py
# `C-c '`  abre o conteudo em outro buffer de forma editavel
#+INCLUDE: "../../src/run_tests.py" src python
***** Script responsavel pelo ping - run_ping_test.py
# `C-c '`  abre o conteudo em outro buffer de forma editavel
#+INCLUDE: "../../src/run_ping_test.py" src python
***** Scripts que define as funcões que leem o env - env_loader.py
# `C-c '`  abre o conteudo em outro buffer de forma editavel
#+INCLUDE: "../../src/utils/env_loader.py" src python
***** Scripts que define as funcões que gerenciam a compressão dos dados coletados - env_loader.py
# `C-c '`  abre o conteudo em outro buffer de forma editavel
#+INCLUDE: "../../src/utils/csv_gzip_manager.py" src python
  
** Funcoes para plotar os graficos
Como alguns gráficos serão usados em mais de um lugar mas com a fonte dos dados diferente, deixaremos esses gráficos encapsulados em funcoes que permitem o facil reuso.

*** Setup

Bloco que importa as biblioteca necessárias e define as funcoes de importacao dos dados

#+BEGIN_SRC python :eval yes :exports :results output :session graphs
  import sys
  import pandas as pd
  import os
  import matplotlib.pyplot as plt
  import matplotlib.dates as mdates
  import seaborn as sns
  import gzip
  from scipy.stats import norm

  SCRIPT_DIR = ORGDIR 
  PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, "..", ".."))
  DATA_ROOT = os.path.join(PROJECT_ROOT, "data")
  DATE_FMT = mdates.DateFormatter("%d/%m")


  def _load_csv_or_gzip(path: str):
      gz_path = path + ".gz"

      if os.path.exists(gz_path):
          with gzip.open(gz_path, "rt", encoding="utf-8") as f:
              return pd.read_csv(f)
      elif os.path.exists(path):
          return pd.read_csv(path)
      else:
          raise FileNotFoundError(f"No CSV or GZIP found for: {path}")

  def load_ping_data(user):
      csv = f"{DATA_ROOT}/{user}/ping_results.csv"
      df = _load_csv_or_gzip(csv)
      df["timestamp"] = pd.to_datetime(
          df["timestamp"],
          format="ISO8601",
          errors="raise"
      )
      return df

  def load_iperf_data(user):
      csv = f"{DATA_ROOT}/{user}/iperf_results.csv"
      df = _load_csv_or_gzip(csv)
      df["timestamp"] = pd.to_datetime(
          df["timestamp"],
          format="ISO8601",
          errors="raise"
      )
      return df

  def load_iperf_rev_data(user):
      csv = f"{DATA_ROOT}/{user}/iperf_results_reversed.csv"
      df = _load_csv_or_gzip(csv)
      df["timestamp"] = pd.to_datetime(
          df["timestamp"],
          format="ISO8601",
          errors="raise"
      )
      return df

  def ensure_img_dir(user):
      d = f"{PROJECT_ROOT}/data/{user}/images"
      os.makedirs(d, exist_ok=True)
      return d

  def last_value_per_session(df, column):
      df_sorted = df.sort_values("timestamp")
      return (
          df_sorted.groupby(["test_label", "session"])
          .tail(1)[["timestamp", "test_label", column]]
          .copy()
      )

  def label_line_end(ax, line, label=None, **kwargs):
      x, y = line.get_xdata(), line.get_ydata()
      label_text = label

      xmin, xmax = ax.get_xlim()
      xmax += (xmax - xmin) * 0.05
      ax.set_xlim(xmin, xmax)

      ax.text(
          x[-1],
          y[-1],
          f" {label_text}",
          color=line.get_color(),
          fontsize=10,
          va="center",
          ha="left",
          transform=ax.transData,
          ,**kwargs,
      )
#+END_SRC

#+RESULTS:

*** Jitter over time
#+NAME: plot_jitter_over_time
#+BEGIN_SRC python :var user="gabriel" :var image_size='(7 3) :var ci_level=0.95 :var combined_graph="true" :results file graphics :session graphs :exports both
    df = load_ping_data(user)
    img_dir = ensure_img_dir(user)

    df = last_value_per_session(df, "ping_jitter_ms")
    df = df[df["ping_jitter_ms"].notna()].copy()

    
    df["lb"] = df.groupby("test_label")["ping_jitter_ms"].transform(lambda x: x.quantile(0.01))
    df["ub"] = df.groupby("test_label")["ping_jitter_ms"].transform(lambda x: x.quantile(0.99))
    df = df[(df["ping_jitter_ms"] >= df["lb"]) & (df["ping_jitter_ms"] <= df["ub"])]
    df = df.drop(columns=["lb","ub"])

    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df["hour"] = df["timestamp"].dt.floor("H")

    z = norm.ppf((1 + ci_level) / 2)

    hourly = (
        df.groupby(["test_label", "hour"])
        .agg(mean=("ping_jitter_ms","mean"),
             std=("ping_jitter_ms","std"),
             count=("ping_jitter_ms","count"))
        .reset_index()
    )

    hourly["stderr"] = hourly["std"] / hourly["count"].pow(0.5)
    hourly["ci"] = z * hourly["stderr"]

    labels = ["VPN_ON","VPN_OFF"]

    ci_label = f"ci{int(ci_level*100)}"

    if combined_graph:
        fig, ax = plt.subplots(figsize=image_size)
        axes = [ax]
    else:
        fig, axes = plt.subplots(1, 2, figsize=image_size)

    for i, label in enumerate(labels):
        ax = axes[0] if combined_graph else axes[i]
        
        subset = hourly[hourly["test_label"] == label].sort_values("hour")

        line, = ax.plot(subset["hour"], subset["mean"])
        ax.fill_between(
            subset["hour"],
            subset["mean"] - subset["ci"],
            subset["mean"] + subset["ci"],
            alpha=0.25,
            label=ci_label
            )

        ax.tick_params(axis='x', rotation=45)
        ax.xaxis.set_major_formatter(DATE_FMT)
        ax.xaxis.set_major_locator(mdates.DayLocator())
        ax.set_title(f"Jitter over time – {label}")
        ax.set_xlabel("Date")
        ax.set_ylabel("ms")
        ax.grid(alpha=0.3)
        ax.legend(loc="best", fontsize="small")
        label_line_end(ax, line, label)

    for ax in axes:
        ax.relim()
        ax.autoscale_view()
        ax.set_ylim(bottom=0)

    path = f"{img_dir}/jitter_over_time_{user}.png"
    plt.tight_layout()
    plt.savefig(path, dpi=150)
    plt.close()

    path
#+END_SRC

#+RESULTS: plot_jitter_over_time
[[file:/home/gabip/perf/data/gabriel/images/jitter_over_time_gabriel.png]]

*** Jitter per hour
#+NAME: plot_jitter_per_hour
#+BEGIN_SRC python :var user="gabriel" :var image_size='(7 3) :var ci_level=0.95 :var combined_graph="true" :results file graphics :session graphs :exports both
    df = load_ping_data(user)
    img_dir = ensure_img_dir(user)

    df = last_value_per_session(df, "ping_jitter_ms")

    df = df[df["ping_jitter_ms"].notna()].copy()
    df["lb"] = df.groupby("test_label")["ping_jitter_ms"].transform(lambda x: x.quantile(0.01))
    df["ub"] = df.groupby("test_label")["ping_jitter_ms"].transform(lambda x: x.quantile(0.99))
    df = df[(df["ping_jitter_ms"] >= df["lb"]) & (df["ping_jitter_ms"] <= df["ub"])]
    df = df.drop(columns=["lb","ub"])

    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df["hour"] = df["timestamp"].dt.hour + 1

    z = norm.ppf((1 + ci_level) / 2)

    hourly = (
        df.groupby(["test_label", "hour"])
        .agg(mean=("ping_jitter_ms", "mean"),
                std=("ping_jitter_ms", "std"),
                count=("ping_jitter_ms", "count"))
        .reset_index()
    )

    hourly["stderr"] = hourly["std"] / hourly["count"].pow(0.5)
    hourly["ci"] = z * hourly["stderr"]

    labels = ["VPN_ON","VPN_OFF"]

    ci_label = f"ci{int(ci_level*100)}"

    if combined_graph:
        fig, ax = plt.subplots(figsize=image_size)
        axes = [ax]
    else:
        fig, axes = plt.subplots(1, 2, figsize=image_size)

    for i, label in enumerate(labels):
        ax = axes[0] if combined_graph else axes[i]
        
        subset = hourly[hourly["test_label"] == label].sort_values("hour")

        line, = ax.plot(subset["hour"], subset["mean"])
        ax.fill_between(
            subset["hour"],
            subset["mean"] - subset["ci"],
            subset["mean"] + subset["ci"],
            alpha=0.25, 
            label=ci_label
            )

        ax.set_title(f"Jitter per hour – {label}")
        ax.set_xlabel("Hour")
        ax.set_xticks(range(0,24,2))
        ax.grid(alpha=0.3)
        ax.set_xlim(1,24)
        if ax == axes[0]:
            ax.set_ylabel("ms")

        ax.legend(loc="best", fontsize="small")
        label_line_end(ax, line, label)

    for ax in axes:
        ax.relim()
        ax.autoscale_view()
        ax.set_ylim(bottom=0)

    path = f"{img_dir}/jitter_per_hour_{user}.png"
    plt.tight_layout()
    plt.savefig(path, dpi=150)
    plt.close()

    path
#+END_SRC

#+RESULTS: plot_jitter_per_hour
[[file:/home/gabip/perf/data/gabriel/images/jitter_per_hour_gabriel.png]]

#+RESULTS: plot_jitter
[[file:/home/gabip/perf/gabriel/images/jitter_gabriel.png]]


#+NAME: plot_latency_per_hour
#+BEGIN_SRC python :var user="gabriel" :var image_size='(9 3) :var ci_level=0.95 :var combined_graph="true" :results file graphics :session graphs :exports both
    import pandas as pd
    import matplotlib.pyplot as plt
    from scipy.stats import norm

    df = load_ping_data(user)
    img_dir = ensure_img_dir(user)

    df = df[df["ping_latency_ms"].notna()].copy()
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df["hour"] = df["timestamp"].dt.hour

    z = norm.ppf((1 + ci_level) / 2)

    hourly = (
        df.groupby(["test_label", "hour"])["ping_latency_ms"]
        .agg(["mean", "count", "std"])
        .reset_index()
    )

    hourly["stderr"] = hourly["std"] / hourly["count"].pow(0.5)
    hourly["ci"] = z * hourly["stderr"]

    labels = ["VPN_ON", "VPN_OFF"]

    ci_label = f"ci{int(ci_level*100)}"

    if combined_graph:
        fig, ax = plt.subplots(figsize=image_size)
        axes = [ax]
    else:
        fig, axes = plt.subplots(1, 2, figsize=image_size)

    for i, label in enumerate(labels):
        ax = axes[0] if combined_graph else axes[i]

        subset = hourly[hourly["test_label"] == label].sort_values("hour")

        line, = ax.plot(subset["hour"], subset["mean"])
        ax.fill_between(
            subset["hour"],
            subset["mean"] - subset["ci"],
            subset["mean"] + subset["ci"],
            alpha=0.25,
            label=ci_label
        )

        ax.set_title(f"Latency per hour of day – {label}")
        ax.set_xlabel("Hour of day")
        ax.set_ylabel("ms")
        ax.set_xticks(range(0, 24, 2))
        ax.grid(alpha=0.3)
        ax.set_xlim(0, 23)
        ax.legend(loc="best", fontsize="small")
        label_line_end(ax, line, label)

    for ax in axes:
        ax.relim()
        ax.autoscale_view()
        ax.set_ylim(bottom=0)

    path = f"{img_dir}/latency_per_hour_{user}.png"
    plt.tight_layout()
    plt.savefig(path, dpi=150)
    plt.close()

    path
#+END_SRC

*** Latencia ao longo do tempo
#+NAME: plot_latency_over_time
#+BEGIN_SRC python :var user="gabriel" :var image_size='(6 4) :var ci_level=0.95 :var combined_graph="true" :results file graphics :session graphs :exports both
    df = load_ping_data(user)
    img_dir = ensure_img_dir(user)

    df = df[df["ping_latency_ms"].notna()].copy()
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df["hour"] = df["timestamp"].dt.floor("H")

    z = norm.ppf((1 + ci_level) / 2)

    hourly = (
        df.groupby(["test_label", "hour"])
        .agg(
            mean=("ping_latency_ms", "mean"),
            std=("ping_latency_ms", "std"),   
            count=("ping_latency_ms", "count"),
        )
        .reset_index()
    )

    hourly["stderr"] = hourly["std"] / hourly["count"].pow(0.5)
    hourly["ci"] = z * hourly["stderr"]

    labels = ["VPN_ON", "VPN_OFF"]

    ci_label = f"ci{int(ci_level*100)}"

    if combined_graph:
        fig, ax = plt.subplots(figsize=image_size)
        axes = [ax]
    else:
        fig, axes = plt.subplots(1, 2, figsize=image_size)

    for i, label in enumerate(labels):
        ax = axes[0] if combined_graph else axes[i]

        subset = hourly[hourly["test_label"] == label].sort_values("hour")

        line, = ax.plot(subset["hour"], subset["mean"])
        ax.fill_between(
            subset["hour"],
            subset["mean"] - subset["ci"],
            subset["mean"] + subset["ci"],
            alpha=0.25,
            label=ci_label
        )

        ax.tick_params(axis='x', rotation=45)
        ax.xaxis.set_major_formatter(DATE_FMT)
        ax.xaxis.set_major_locator(mdates.DayLocator())
        ax.set_title(f"Latency per hour of day – {label}")
        ax.set_xlabel("Hour of day")
        ax.set_ylabel("ms")
        ax.grid(alpha=0.3)
        ax.legend(loc="best", fontsize="small")
        label_line_end(ax, line, label)

    for ax in axes:
        ax.relim()
        ax.autoscale_view()
        ax.set_ylim(bottom=0)

    path = f"{img_dir}/latency_over_time_{user}.png"
    plt.tight_layout()
    plt.savefig(path, dpi=150)
    plt.close()

    path
#+END_SRC

#+RESULTS: plot_latency_over_time
[[file:/home/gabip/perf/data/gabriel/images/latency_over_time_gabriel.png]]

*** Latencia por hora

#+NAME: plot_latency_per_hour
#+BEGIN_SRC python :var user="gabriel" :var image_size='(9 3) :var ci_level=0.95 :var combined_graph="true" :results file graphics :session graphs :exports both
    df = load_ping_data(user)
    img_dir = ensure_img_dir(user)

    df["lb"] = df.groupby("test_label")["ping_latency_ms"].transform(lambda x: x.quantile(0.01))
    df["ub"] = df.groupby("test_label")["ping_latency_ms"].transform(lambda x: x.quantile(0.99))
    df = df[(df["ping_latency_ms"] >= df["lb"]) & (df["ping_latency_ms"] <= df["ub"])]
    df = df.drop(columns=["lb", "ub"])

    df = df[df["ping_latency_ms"].notna()].copy()
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df["hour"] = df["timestamp"].dt.hour + 1

    z = norm.ppf((1 + ci_level) / 2)

    hourly = (
        df.groupby(["test_label", "hour"])
        .agg(
            mean=("ping_latency_ms", "mean"),
            std=("ping_latency_ms", "std"),   
            count=("ping_latency_ms", "count"),
        )
        .reset_index()
    )


    hourly["stderr"] = hourly["std"] / hourly["count"].pow(0.5)
    hourly["ci"] = z * hourly["stderr"]

    labels = ["VPN_ON", "VPN_OFF"]

    ci_label = f"ci{int(ci_level*100)}"

    if combined_graph:
        fig, ax = plt.subplots(figsize=image_size)
        axes = [ax]
    else:
        fig, axes = plt.subplots(1, 2, figsize=image_size)

    for i, label in enumerate(labels):
        ax = axes[0] if combined_graph else axes[i]
            
        subset = hourly[hourly["test_label"] == label].sort_values("hour")

        line, = ax.plot(subset["hour"], subset["mean"])
        ax.fill_between(
            subset["hour"],
            subset["mean"] - subset["ci"],
            subset["mean"] + subset["ci"],
            alpha=0.25,
            label=ci_label
        )

        ax.set_title(f"Latency per hour of day – {label}")
        ax.set_xlabel("Hour of day")
        ax.set_xticks(range(0, 24, 2))
        ax.grid(alpha=0.3)
        ax.set_xlim(1, 24)
        if ax == axes[0]:
            ax.set_ylabel("ms")
        ax.legend(loc="best", fontsize="small")
        label_line_end(ax, line, label)

    for ax in axes:
        ax.relim()
        ax.autoscale_view()
        ax.set_ylim(bottom=0)

    path = f"{img_dir}/latency_per_hour_{user}.png"
    plt.tight_layout()
    plt.savefig(path, dpi=150)
    plt.close()

    path
#+END_SRC

#+RESULTS: plot_latency_per_hour
[[file:/home/gabip/perf/data/gabriel/images/latency_per_hour_gabriel.png]]

*** Upload ao longo do tempo

#+NAME: plot_upload_over_time
#+BEGIN_SRC python :var user="gabriel" :var image_size='(9 3) :var ci_level=0.95 :var combined_graph="true" :results file graphics :session graphs :exports both
    df = load_iperf_data(user)
    img_dir = ensure_img_dir(user)

    df_t = df[df["tcp_throughput_mbps"].notna()].copy()
    df_t["timestamp"] = pd.to_datetime(df_t["timestamp"])
    df_t["hour"] = df_t["timestamp"].dt.floor("H")

    z = norm.ppf((1 + ci_level) / 2)

    hourly = (
        df_t.groupby(["test_label", "hour"])["tcp_throughput_mbps"]
        .agg(['mean', 'count', 'std'])
        .reset_index()
    )

    hourly["stderr"] = hourly["std"] / hourly["count"].pow(0.5)
    hourly["ci"] = z * hourly["stderr"]


    labels = ["VPN_ON", "VPN_OFF"]

    ci_label = f"ci{int(ci_level*100)}"

    if combined_graph:
        fig, ax = plt.subplots(figsize=image_size)
        axes = [ax]
    else:
        fig, axes = plt.subplots(1, 2, figsize=image_size)

    for i, label in enumerate(labels):
        ax = axes[0] if combined_graph else axes[i]

        subset = hourly[hourly["test_label"] == label]

        line, = ax.plot(subset["hour"], subset["mean"])
        ax.fill_between(
            subset["hour"],
            subset["mean"] - subset["ci"],
            subset["mean"] + subset["ci"],
            alpha=0.25,
            label=ci_label
        )
        ax.tick_params(axis='x', rotation=45)
        ax.xaxis.set_major_formatter(DATE_FMT)
        ax.xaxis.set_major_locator(mdates.DayLocator())
        ax.tick_params(axis='x', rotation=45)
        ax.set_xlabel("Date")
        ax.set_ylabel("Mbps")
        ax.set_title(f"Upload over time – {label}")
        ax.set_ylim(bottom = 0)
        ax.legend(loc="best", fontsize="small")
        label_line_end(ax, line, label)

    for ax in axes:
        ax.relim()
        ax.autoscale_view()
        ax.set_ylim(bottom=0)


    path = f"{img_dir}/upload_over_time_{user}.png"
    plt.tight_layout()
    plt.savefig(path, dpi=150)
    plt.close()

    path
#+END_SRC

#+RESULTS: plot_upload_over_time
[[file:/home/gabip/perf/data/gabriel/images/upload_over_time_gabriel.png]]

*** Download ao longo do tempo 

#+NAME: plot_download_over_time
#+BEGIN_SRC python :var user="gabriel" :var image_size='(9 3) :var ci_level=0.95 :var combined_graph="true" :results file graphics :session graphs :exports both
    df = load_iperf_rev_data(user)
    img_dir = ensure_img_dir(user)

    df_t = df[df["tcp_throughput_mbps"].notna()].copy()
    df_t["timestamp"] = pd.to_datetime(df_t["timestamp"])
    df_t["hour"] = df_t["timestamp"].dt.floor("H")

    z = norm.ppf((1 + ci_level) / 2)

    hourly = (
        df_t.groupby(["test_label", "hour"])["tcp_throughput_mbps"]
        .agg(['mean', 'count', 'std'])
        .reset_index()
    )

    hourly["stderr"] = hourly["std"] / hourly["count"].pow(0.5)
    hourly["ci"] = z * hourly["stderr"]


    labels = ["VPN_ON", "VPN_OFF"]

    ci_label = f"ci{int(ci_level*100)}"

    if combined_graph:
        fig, ax = plt.subplots(figsize=image_size)
        axes = [ax]
    else:
        fig, axes = plt.subplots(1, 2, figsize=image_size)

    for i, label in enumerate(labels):
        ax = axes[0] if combined_graph else axes[i]

        subset = hourly[hourly["test_label"] == label]

        line, = ax.plot(subset["hour"], subset["mean"])
        ax.fill_between(
            subset["hour"],
            subset["mean"] - subset["ci"],
            subset["mean"] + subset["ci"],
            alpha=0.25,
            label=ci_label
        )
        ax.tick_params(axis='x', rotation=45)
        ax.xaxis.set_major_formatter(DATE_FMT)
        ax.xaxis.set_major_locator(mdates.DayLocator())
        ax.tick_params(axis='x', rotation=45)
        ax.set_xlabel("Date")
        ax.set_ylabel("Mbps")
        ax.set_title(f"Download over time  – {label}")
        ax.legend(loc="best", fontsize="small")
        label_line_end(ax, line, label)

    for ax in axes:
        ax.relim()
        ax.autoscale_view()
        ax.set_ylim(bottom=0)

    path = f"{img_dir}/download_over_time_{user}.png"
    plt.tight_layout()
    plt.savefig(path, dpi=150)
    plt.close()

    path
#+END_SRC

#+RESULTS: plot_download_over_time
[[file:/home/gabip/perf/data/gabriel/images/download_over_time_gabriel.png]]

*** Upload por hora do dia  

#+NAME: plot_upload_per_hour
#+BEGIN_SRC python :var user="gabriel" :var image_size='(9 3) :var ci_level=0.95 :var combined_graph="true" :results file graphics :session graphs :exports both
    df = load_iperf_data(user)
    img_dir = ensure_img_dir(user)

    df = df[df["tcp_throughput_mbps"].notna()].copy()
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df["hour"] = df["timestamp"].dt.hour   

    z = norm.ppf((1 + ci_level) / 2)

    hourly = (
        df.groupby(["test_label", "hour"])["tcp_throughput_mbps"]
        .agg(["mean", "count", "std"])
        .reset_index()
    )

    hourly["stderr"] = hourly["std"] / hourly["count"].pow(0.5)
    hourly["ci"] = z * hourly["stderr"]

    labels = ["VPN_ON", "VPN_OFF"]
    
    ci_label = f"ci{int(ci_level*100)}"

    if combined_graph:
        fig, ax = plt.subplots(figsize=image_size)
        axes = [ax]
    else:
        fig, axes = plt.subplots(1, 2, figsize=image_size)

    for i, label in enumerate(labels):
        ax = axes[0] if combined_graph else axes[i]

        subset = hourly[hourly["test_label"] == label].sort_values("hour")

        line, = ax.plot(subset["hour"], subset["mean"])
        ax.fill_between(
            subset["hour"],
            subset["mean"] - subset["ci"],
            subset["mean"] + subset["ci"],
            alpha=0.25,
            label=ci_label
        )
        
        ax.set_title(f"Upload per hour of day – {label}")
        ax.set_xlabel("Hour of day")
        ax.set_ylabel("Mbps")
        ax.set_xticks(range(0, 24, 2))
        ax.set_xlim(1, 24)
        ax.grid(alpha=0.3)
        ax.legend(loc="best", fontsize="small")
        label_line_end(ax, line, label)

    for ax in axes:
        ax.relim()
        ax.autoscale_view()
        ax.set_ylim(bottom=0)

    path = f"{img_dir}/upload_per_hour_{user}.png"
    plt.tight_layout()
    plt.savefig(path, dpi=150)
    plt.close()

    path
#+END_SRC

#+RESULTS: plot_upload_per_hour
[[file:/home/gabip/perf/data/gabriel/images/upload_per_hour_gabriel.png]]

*** Download por hora do dia  

#+NAME: plot_download_per_hour
#+BEGIN_SRC python :var user="gabriel" :var image_size='(9 3) :var ci_level=0.95 :var combined_graph="true" :results file graphics :session graphs :exports both
    df = load_iperf_rev_data(user)
    img_dir = ensure_img_dir(user)

    df = df[df["tcp_throughput_mbps"].notna()].copy()
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df["hour"] = df["timestamp"].dt.hour + 1

    z = norm.ppf((1 + ci_level) / 2)

    hourly = (
        df.groupby(["test_label", "hour"])["tcp_throughput_mbps"]
        .agg(["mean", "count", "std"])
        .reset_index()
    )

    hourly["stderr"] = hourly["std"] / hourly["count"].pow(0.5)
    hourly["ci"] = z * hourly["stderr"]

    labels = ["VPN_ON", "VPN_OFF"]
    
    ci_label = f"ci{int(ci_level*100)}"

    if combined_graph:
        fig, ax = plt.subplots(figsize=image_size)
        axes = [ax]
    else:
        fig, axes = plt.subplots(1, 2, figsize=image_size)

    for i, label in enumerate(labels):
        ax = axes[0] if combined_graph else axes[i]

        subset = hourly[hourly["test_label"] == label].sort_values("hour")

        line, = ax.plot(subset["hour"], subset["mean"])
        ax.fill_between(
            subset["hour"],
            subset["mean"] - subset["ci"],
            subset["mean"] + subset["ci"],
            alpha=0.25,
            label=ci_label
        )

        ax.set_title(f"Download per hour of day – {label}")
        ax.set_xlabel("Hour of day")
        ax.set_ylabel("Mbps")
        ax.set_xticks(range(0, 24, 2))
        ax.grid(alpha=0.3)
        ax.set_xlim(1, 24)
        ax.legend(loc="best", fontsize="small")
        label_line_end(ax, line, label)

    for ax in axes:
        ax.relim()
        ax.autoscale_view()
        ax.set_ylim(bottom=0)

    path = f"{img_dir}/download_per_hour_{user}.png"
    plt.tight_layout()
    plt.savefig(path, dpi=150)
    plt.close()

    path
#+END_SRC

#+RESULTS: plot_download_per_hour
[[file:/home/gabip/perf/data/gabriel/images/download_per_hour_gabriel.png]]


** Regressão linear
    Apesar de dos dados serem series temporais, e o framework visto em aula não suportar esse tipo de análise, vamos aplicá-lo como forma de exercício.

    #+CALL: plot_latency_per_hour(user="gabriel") :session graphs
    
    #+RESULTS:
    [[file:/home/gabip/perf/data/gabriel/images/latency_per_hour_gabriel.png]]

    Vamos tenta explicar a latência em funcão da VPN estar ligada ou não.
    
    #+BEGIN_SRC python :exports :results output :session graphs
      import statsmodels.formula.api as smf

      df = load_ping_data("gabriel")

      print(df.head())

      reg = smf.ols("ping_latency_ms ~ test_label", data=df).fit()
      print(reg.summary())
  #+END_SRC

  #+RESULTS:
  #+begin_example
                     timestamp  session test_label  ping_latency_ms  ping_jitter_ms  ping_loss_%
  0 2025-11-12 22:19:59.276110        1     VPN_ON             15.1            0.00          0.0
  1 2025-11-12 22:20:00.252454        1     VPN_ON             10.6            0.28          0.0
  2 2025-11-12 22:20:01.252663        1     VPN_ON             11.0            0.29          0.0
  3 2025-11-12 22:20:02.261960        1     VPN_ON             20.8            0.88          0.0
  4 2025-11-12 22:20:03.251324        1     VPN_ON             11.8            1.39          0.0
                              OLS Regression Results                            
  ==============================================================================
  Dep. Variable:        ping_latency_ms   R-squared:                       0.030
  Model:                            OLS   Adj. R-squared:                  0.030
  Method:                 Least Squares   F-statistic:                 1.754e+04
  Date:                Sun, 30 Nov 2025   Prob (F-statistic):               0.00
  Time:                        17:26:21   Log-Likelihood:            -3.5501e+06
  No. Observations:              571302   AIC:                         7.100e+06
  Df Residuals:                  571300   BIC:                         7.100e+06
  Df Model:                           1                                         
  Covariance Type:            nonrobust                                         
  ========================================================================================
                             coef    std err          t      P>|t|      [0.025      0.975]
  ----------------------------------------------------------------------------------------
  Intercept               91.5325      0.228    402.024      0.000      91.086      91.979
  test_label[T.VPN_ON]   -42.3784      0.320   -132.433      0.000     -43.006     -41.751
  ==============================================================================
  Omnibus:                   736217.749   Durbin-Watson:                   0.666
  Prob(Omnibus):                  0.000   Jarque-Bera (JB):        492757964.398
  Skew:                           6.700   Prob(JB):                         0.00
  Kurtosis:                     146.251   Cond. No.                         2.63
  ==============================================================================

  Notes:
  [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
  #+end_example

  Obviamente a latência não é explicada pela VPN estar sendo usada ou não, podemos ver pelo R-squared, 0.030.

  Mas esse é um exemplo de como poderíamos fazer a análise caso fosse aplicável ao nosso trabalho.
  
* Apresentacao
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:

** Objetivos do trabalho 
- Avaliar o impacto da infraestrutura de VPN da UFRGS

** Metodologia
Execução simultânea de ping e iperf através de um script que alterna entre:
    - Teste com a conexão via OpenVPN UFRGS.
    - Teste com conexão direta.

** Dados coletados
- Coleta automática de: latência, jitter, perda de pacotes (ping), throughput TCP e UDP (iperf3)
- Dados coletados por integrante do grupo, disponiveis dentro da pasta data
- Arquivos: 
    - =ping_results.csv=
    - =iperf_results.csv=
    - =iperf_results_reversed.csv=

** Ambiente de testes
- Servidor iperf: pcad.inf.ufrgs.br
- Site para ping: moodle.ufrgs.br

Autenticação automática via arquivo pass.txt
Intervalos entre testes: 5 segundos
Duração de cada medição: 10 segundos

- Testes realizados em diferentes dias e horários para melhor amostragem

** Material para medição
# Adicionar Script em python utilizado para solicitar as medidas #

** Resultados preliminares:
- Gráficos

** Próximos passos:
- Realizar mais medições em diferentes horários
- Gerar novas análises e indicadoresP
- Preparar análise crítica para etapa final

** Slide 3 exemplo para imagem

#+CALL: plot_latency(user="gabriel", image_size='(7 3)) :session graphs
#+ATTR_LATEX: :width 1\textwidth
#+RESULTS:
[[file:/home/gabip/perf/gabriel/images/latency_gabriel.png]]
